{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbed43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting EbookLib\n",
      "  Downloading EbookLib-0.18.tar.gz (115 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting lxml (from EbookLib)\n",
      "  Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from EbookLib) (1.16.0)\n",
      "Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.6/3.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 12.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: EbookLib\n",
      "  Building wheel for EbookLib (setup.py): started\n",
      "  Building wheel for EbookLib (setup.py): finished with status 'done'\n",
      "  Created wheel for EbookLib: filename=EbookLib-0.18-py3-none-any.whl size=38804 sha256=f5bc5b61eb8c088dcaf04beb3dc676a9c62128181edc9e54c49e072ff10205ad\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\65\\2b\\63\\68307c736d5a2fafeebe9df3e5eccacfe892204ce1fd31a03c\n",
      "Successfully built EbookLib\n",
      "Installing collected packages: lxml, EbookLib\n",
      "Successfully installed EbookLib-0.18 lxml-5.3.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.0\n"
     ]
    }
   ],
   "source": [
    "!pip install EbookLib\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9435d138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.9/24.0 MB 15.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 15.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.7/24.0 MB 15.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.5/24.0 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.9/24.0 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 14.1 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 3.1/15.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 3.1/15.8 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.8/15.8 MB 9.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/15.8 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/46.2 MB 18.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 7.1/46.2 MB 18.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 10.7/46.2 MB 18.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.2/46.2 MB 17.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 18.1/46.2 MB 17.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 21.8/46.2 MB 17.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.4/46.2 MB 17.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.4/46.2 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 33.0/46.2 MB 17.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.4/46.2 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 17.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.5/46.2 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 16.2 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.0.5 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf9b366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FlagEmbedding\n",
      "  Downloading FlagEmbedding-1.3.2.tar.gz (177 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch>=1.6.0 (from FlagEmbedding)\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers==4.44.2 (from FlagEmbedding)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets==2.19.0 (from FlagEmbedding)\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.20.1 (from FlagEmbedding)\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence_transformers (from FlagEmbedding)\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting peft (from FlagEmbedding)\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ir-datasets (from FlagEmbedding)\n",
      "  Downloading ir_datasets-0.5.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from FlagEmbedding)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting protobuf (from FlagEmbedding)\n",
      "  Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting filelock (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from datasets==2.19.0->FlagEmbedding) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from datasets==2.19.0->FlagEmbedding) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from datasets==2.19.0->FlagEmbedding) (4.67.0)\n",
      "Collecting xxhash (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading aiohttp-3.11.0-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from datasets==2.19.0->FlagEmbedding) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from datasets==2.19.0->FlagEmbedding) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from transformers==4.44.2->FlagEmbedding) (2024.11.6)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.44.2->FlagEmbedding)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2->FlagEmbedding)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from torch>=1.6.0->FlagEmbedding) (4.11.0)\n",
      "Collecting networkx (from torch>=1.6.0->FlagEmbedding)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch>=1.6.0->FlagEmbedding)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.6.0->FlagEmbedding)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from ir-datasets->FlagEmbedding) (4.12.3)\n",
      "Collecting inscriptis>=2.2.0 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading inscriptis-2.5.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: lxml>=4.5.2 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from ir-datasets->FlagEmbedding) (5.3.0)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading lz4-4.3.3-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting zlib-state>=0.1.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading zlib_state-0.1.9-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting ijson>=3.1.3 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading ijson-3.3.0-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir-datasets->FlagEmbedding)\n",
      "  Downloading unlzw3-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting scikit-learn (from sentence_transformers->FlagEmbedding)\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from sentence_transformers->FlagEmbedding) (1.13.1)\n",
      "Collecting Pillow (from sentence_transformers->FlagEmbedding)\n",
      "  Downloading pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.5)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from aiohttp->datasets==2.19.0->FlagEmbedding) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.19.0->FlagEmbedding) (0.4.6)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.3)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.19.0->FlagEmbedding)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from pandas->datasets==2.19.0->FlagEmbedding) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from pandas->datasets==2.19.0->FlagEmbedding) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.19.0->FlagEmbedding)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers->FlagEmbedding)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\ud_intro\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0->FlagEmbedding) (1.16.0)\n",
      "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 542.0/542.0 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/9.5 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.5 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.5 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/203.1 MB 14.0 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 5.8/203.1 MB 14.1 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 9.2/203.1 MB 14.6 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 12.3/203.1 MB 14.9 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 15.5/203.1 MB 15.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 18.6/203.1 MB 15.1 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 22.0/203.1 MB 15.5 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 25.2/203.1 MB 15.5 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 28.6/203.1 MB 15.6 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 32.0/203.1 MB 15.6 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 35.4/203.1 MB 15.7 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 38.5/203.1 MB 15.7 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 41.9/203.1 MB 15.7 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 45.1/203.1 MB 15.8 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 48.5/203.1 MB 15.8 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 51.9/203.1 MB 15.8 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 55.6/203.1 MB 15.9 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 57.9/203.1 MB 15.7 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 61.3/203.1 MB 15.8 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 64.5/203.1 MB 15.8 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 68.4/203.1 MB 15.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 71.8/203.1 MB 15.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 75.0/203.1 MB 15.9 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 78.9/203.1 MB 16.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 82.8/203.1 MB 16.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 86.5/203.1 MB 16.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 90.2/203.1 MB 16.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 93.8/203.1 MB 16.4 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 98.3/203.1 MB 16.6 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 102.8/203.1 MB 16.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 107.5/203.1 MB 16.9 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 112.2/203.1 MB 17.1 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 115.6/203.1 MB 17.2 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 119.0/203.1 MB 17.1 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 122.9/203.1 MB 17.1 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 127.4/203.1 MB 17.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 131.9/203.1 MB 17.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 136.3/203.1 MB 17.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 139.7/203.1 MB 17.5 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 143.9/203.1 MB 17.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 148.1/203.1 MB 17.6 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 152.6/203.1 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 156.8/203.1 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 160.2/203.1 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 162.3/203.1 MB 17.5 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 164.1/203.1 MB 17.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 166.7/203.1 MB 17.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 169.1/203.1 MB 17.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 172.2/203.1 MB 17.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 175.6/203.1 MB 17.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 178.5/203.1 MB 16.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 182.5/203.1 MB 16.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.4/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 190.6/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.5/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.9/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.3/203.1 MB 16.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 17.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.1/6.2 MB 15.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 14.6 MB/s eta 0:00:00\n",
      "Downloading ir_datasets-0.5.9-py3-none-any.whl (347 kB)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Downloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 11.8 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Downloading aiohttp-3.11.0-cp311-cp311-win_amd64.whl (440 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading ijson-3.3.0-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Downloading inscriptis-2.5.0-py3-none-any.whl (45 kB)\n",
      "Downloading lz4-4.3.3-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Downloading pyarrow-18.0.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/25.1 MB 12.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.5/25.1 MB 12.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.1/25.1 MB 12.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 13.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.9/25.1 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.1 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.8/25.1 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Downloading zlib_state-0.1.9-cp311-cp311-win_amd64.whl (12 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.7/11.6 MB 19.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.9/11.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 19.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 17.3 MB/s eta 0:00:00\n",
      "Downloading pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 18.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 3.9/11.0 MB 19.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.0 MB 16.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 15.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.17.1-cp311-cp311-win_amd64.whl (90 kB)\n",
      "Building wheels for collected packages: FlagEmbedding, warc3-wet-clueweb09, cbor\n",
      "  Building wheel for FlagEmbedding (setup.py): started\n",
      "  Building wheel for FlagEmbedding (setup.py): finished with status 'done'\n",
      "  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.3.2-py3-none-any.whl size=239881 sha256=66d294560e98375c41ba85f4fb81c12a79804b144a555f2743a43de48aeeaf9a\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\21\\87\\fb\\a54279e1f45504d8b8abe317ce0e19af4e732cf45aab64e28a\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py): started\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py): finished with status 'done'\n",
      "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18942 sha256=bb968546a3fca076f2be226844fe5d787aa1b43f330b95590a1f86a63854dd84\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\63\\f9\\dc\\2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
      "  Building wheel for cbor (setup.py): started\n",
      "  Building wheel for cbor (setup.py): finished with status 'done'\n",
      "  Created wheel for cbor: filename=cbor-1.0.0-py3-none-any.whl size=10040 sha256=faad0103e6da529232be8697900b5eb416176646aa0afea206cb9d28432e5810\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\21\\6b\\45\\0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
      "Successfully built FlagEmbedding warc3-wet-clueweb09 cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, sentencepiece, mpmath, ijson, cbor, zlib-state, xxhash, unlzw3, tzdata, trec-car-tools, threadpoolctl, sympy, safetensors, pyarrow-hotfix, pyarrow, protobuf, propcache, Pillow, networkx, multidict, lz4, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, torch, scikit-learn, pandas, multiprocess, inscriptis, huggingface-hub, aiosignal, tokenizers, ir-datasets, aiohttp, accelerate, transformers, sentence_transformers, peft, datasets, FlagEmbedding\n",
      "Successfully installed FlagEmbedding-1.3.2 Pillow-11.0.0 accelerate-1.1.1 aiohappyeyeballs-2.4.3 aiohttp-3.11.0 aiosignal-1.3.1 cbor-1.0.0 datasets-2.19.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.3.1 huggingface-hub-0.26.2 ijson-3.3.0 inscriptis-2.5.0 ir-datasets-0.5.9 lz4-4.3.3 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 pandas-2.2.3 peft-0.13.2 propcache-0.2.0 protobuf-5.28.3 pyarrow-18.0.0 pyarrow-hotfix-0.6 safetensors-0.4.5 scikit-learn-1.5.2 sentence_transformers-3.3.0 sentencepiece-0.2.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.5.1 transformers-4.44.2 trec-car-tools-2.6 tzdata-2024.2 unlzw3-0.2.2 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 xxhash-3.5.0 yarl-1.17.1 zlib-state-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4344df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d39baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb2a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_epub_files(folder_path):\n",
    "    epub_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.epub'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                unique_id = str(uuid.uuid4())\n",
    "                epub_files.append({'file_path': file_path, 'ID': unique_id})\n",
    "    return epub_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d27fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_paragraphs(epub_file, ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    paragraphs = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        content = re.sub('<[^<]+?>', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = re.sub('\\n', ' ', content)\n",
    "        \n",
    "        paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    \n",
    "    paragraphs = merge_strings_until_limit(paragraphs, 200, 1000)\n",
    "    paragraphs = [{'paragraph':paragraphs[i], 'nr':i, 'bookID':ID} for i in range(len(paragraphs))]\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def merge_strings_until_limit(strings, min_length, max_length, test_for_max = 0):\n",
    "    merged_string = \"\"\n",
    "    merged_strings = []\n",
    "    \n",
    "    for s in strings:\n",
    "        if len(merged_string) <= min_length:\n",
    "            merged_string += s\n",
    "        \n",
    "        elif len(merged_string) > max_length and test_for_max<5:\n",
    "                splitParagraph = merged_string.split('.')\n",
    "                splitParagraphRePoint = []\n",
    "                for sp in splitParagraph:\n",
    "                    splitParagraphRePoint.append(sp+'.')\n",
    "                \n",
    "                merged = merge_strings_until_limit(splitParagraphRePoint, min_length, max_length, test_for_max+1)\n",
    "                merged_strings.extend(merged)\n",
    "                merged_string = s\n",
    "        else:\n",
    "            merged_strings.append(merged_string)\n",
    "            merged_string = s\n",
    "    \n",
    "    if merged_string:\n",
    "        merged_strings.append(merged_string)\n",
    "    \n",
    "    return merged_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fc130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a048b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924133e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4507e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473aa2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, stopwords, names\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8567e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ccd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess(text, min_len=3, deacc=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    if deacc:\n",
    "        words = [re.sub(r'[^a-zA-Z0-9\\s]', '', word) for word in words]\n",
    "    \n",
    "    words = [word for word in words if len(word) >= min_len]\n",
    "    return words\n",
    "\n",
    "def process_paragraphs(paragraphs):\n",
    "    processed_paragraphs = []\n",
    "    \n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        \n",
    "        words = simple_preprocess(paragraph['paragraph'], min_len=3, deacc=True)\n",
    "        \n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        filtered_words = [word for word in lemmatized_words if word.lower() not in STOP_WORDS and word.isalpha()]\n",
    "        \n",
    "        stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        processed_paragraph = \" \".join(stemmed_words)\n",
    "        processed_paragraphs.append({'paragraph': processed_paragraph, 'nr': paragraph['nr'], 'bookID': paragraph['bookID']})\n",
    "    \n",
    "    return processed_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b371174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(phrase):\n",
    "    \n",
    "    query = [{'paragraph': phrase, 'nr': 1, 'bookID': 'book1'}]\n",
    "    \n",
    "    processedQuery = process_paragraphs(query)\n",
    "    \n",
    "    processedQuery = [paragraph['paragraph'] for paragraph in processedQuery]\n",
    "    \n",
    "    return processedQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0407b90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da5de5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbor(processed_paragraphs, processedQuery):\n",
    "\n",
    "    processed_docs = [paragraph['paragraph'] for paragraph in processed_paragraphs]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "    n_samples, n_features = tfidf_matrix.shape\n",
    "    print(f\"The TF-IDF matrix has {n_samples} samples and {n_features} features.\")\n",
    "    \n",
    "    query_vector = vectorizer.transform(processedQuery)\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    nearest_neighbor_index = similarities.argmax()\n",
    "    \n",
    "    nearest_neighbor = paragraphs[nearest_neighbor_index]\n",
    "    \n",
    "    similarity_score = similarities[nearest_neighbor_index]\n",
    "    \n",
    "    top_indices = tfidf_matrix[nearest_neighbor_index].toarray().flatten().argsort()[::-1]\n",
    "    top_terms = [vectorizer.get_feature_names_out()[idx] for idx in top_indices[:10]]\n",
    "    \n",
    "    return nearest_neighbor, similarity_score, top_terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9b14a-b20d-4863-af8c-b5b1b8f2b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bedd73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF matrix has 1066 samples and 2722 features.\n",
      "Nearest Neighbor: {'paragraph': ' But after all Monte Carlo was not a capital; it was just an absurd little joke of a town crammed on a ledge between sea and mountain; and a second glance at the young man convinced the Professor that he was as harmless as the town.', 'nr': 980, 'bookID': '0528bf13-8c9f-4298-a99e-3315b2b57826'}\n",
      "Similarity Score: 0.3208386099054711\n",
      "Top Terms: ['town', 'harmless', 'capit', 'absurd', 'cram', 'convinc', 'mountain', 'joke', 'second', 'sea']\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'C:\\Users\\admin\\epubs'\n",
    "epub_files = read_all_epub_files(folder_path)\n",
    "\n",
    "for epub_file in epub_files:\n",
    "    paragraphs = read_epub_paragraphs(epub_file['file_path'], epub_file['ID'])\n",
    "    \n",
    "search_phrase = 'become one manufacturing town'\n",
    "\n",
    "processed_paragraphs = process_paragraphs(paragraphs)\n",
    "processed_query = process_query(search_phrase)\n",
    "nearest_neighbor, similarity_score, top_terms = find_nearest_neighbor(processed_paragraphs, processed_query)\n",
    "\n",
    "print(\"Nearest Neighbor:\", nearest_neighbor)\n",
    "print(\"Similarity Score:\", similarity_score)\n",
    "print(\"Top Terms:\", top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3476fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f02faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到ID为 112b100f-b93e-48c3-9871-8db5ca032225 的EPUB文件\n"
     ]
    }
   ],
   "source": [
    "def find_epub_file_path(epub_list, target_id):\n",
    "   \n",
    "    for epub_file in epub_list:\n",
    "        if epub_file['ID'] == target_id:\n",
    "            return epub_file['file_path']\n",
    "    return None\n",
    "    \n",
    "target_id = '112b100f-b93e-48c3-9871-8db5ca032225'\n",
    "\n",
    "file_path = find_epub_file_path(epub_files, target_id)\n",
    "if file_path:\n",
    "    print(f\"找到ID为 {target_id} 的EPUB文件，路径为: {file_path}\")\n",
    "else:\n",
    "    print(f\"未找到ID为 {target_id} 的EPUB文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc8e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9389b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for professor:\n",
      " [ 0.00946225  0.02472278 -0.00477998 -0.00037209 -0.06319729 -0.11113457\n",
      "  0.06844848  0.2045158  -0.18921745 -0.0235161   0.03084886 -0.12770267\n",
      "  0.00351073  0.10496946 -0.04789141  0.07648164  0.04403925  0.01836617\n",
      " -0.12558216 -0.09943662  0.03404718  0.11339442  0.18120342 -0.1102364\n",
      "  0.03992439  0.04779569 -0.0588833  -0.06513777 -0.12273138  0.00290025\n",
      "  0.0302754  -0.03009903 -0.06611849  0.01737972 -0.06911599  0.10089195\n",
      "  0.08954611  0.02410895  0.08479051 -0.08009283  0.11067646 -0.01006319\n",
      " -0.07928267  0.00456561  0.16112636  0.00999212 -0.018768   -0.07919914\n",
      "  0.09416746  0.07934054]\n",
      "Words similar to professor: [('wa', 0.9874762296676636), ('said', 0.986337423324585), ('would', 0.9860908389091492)]\n"
     ]
    }
   ],
   "source": [
    "def train_w2v_and_find_similar(processed_paragraphs, target_word, vector_size=50, window=3, min_count=1, workers=4):\n",
    "    \n",
    "    tokenized_sentences = [word_tokenize(sentence['paragraph'].lower()) for sentence in processed_paragraphs]\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    \n",
    "    return w2v_model.wv[target_word], w2v_model.wv.most_similar(target_word, topn=3)\n",
    "\n",
    "\n",
    "word_to_check = 'professor'\n",
    "try:\n",
    "    word_vector, similar_words = train_w2v_and_find_similar(processed_paragraphs, word_to_check)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Vector for {word_to_check}:\\n\", word_vector)\n",
    "    print(f\"Words similar to {word_to_check}:\", similar_words)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95a49bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\UD_intro\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 5/5 [04:54<00:00, 58.99s/it] \n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5',\n",
    "                  query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\")\n",
    "embeddings1 = model.encode([p['paragraph'] for p in paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b942746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e95de56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': ' Not till tomorrow morning at 8:30. To confirm his statement the porter pointed to a large time–table on the wall of the waiting–room. The Professor scanned it and sat down again with a groan. He was about to consult his companion as to the possibility of finding a night\\'s lodging in a respectable pension (fantastic as the idea seemed in such a place); but hardly had he begun: \"Can you tell me where—\" when, with a nod of comprehension and a wink of complicity, the porter returned in fluent English: \"Pretty ladies? Turkish bath? Fottographs?\" The Professor repudiated these suggestions with a shudder, and leaving his bags in the cloak–room set forth on his quest.',\n",
       " 'nr': 971,\n",
       " 'bookID': 'f5fc8a5c-c5fd-4f31-af6a-d81e378442ad'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = model.encode_queries(['shopping for culture'])\n",
    "similarities = embeddings1 @ queries.T\n",
    "top_index = similarities.argmax()\n",
    "paragraphs[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2279f2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4300005 ],\n",
       "       [0.4669631 ],\n",
       "       [0.3892948 ],\n",
       "       ...,\n",
       "       [0.40824968],\n",
       "       [0.3960275 ],\n",
       "       [0.49921626]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828ff9f-e32f-4886-adc0-207c6f98bb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e5a30-a37a-4a3d-a951-d699847587cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f2d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0a979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
